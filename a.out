Finetuning model with params:
base_model: /home/models/nvidia-sparse/OpenReasoning-Nemotron-14B-Sparse-0.67
data_path: /home/aneek/LLM-Adapters/ft-training_set/split_33/math_14k_part1_of_3.json
output_dir: ./trained_models/Nemotron-14B-Sparse/OpenReasoning-Nemotron-14B-Sparse-0.67-Ensemble_split_33/OpenReasoning-Nemotron-14B-Sparse-0.67-Math14k-part1
batch_size: 4
micro_batch_size: 1
num_epochs: 3
learning_rate: 3e-05
cutoff_len: 256
val_set_size: 120
use_gradient_checkpointing: False
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: None
bottleneck_size: 256
non_linearity: tanh
adapter_dropout: 0.0
use_parallel_adapter: False
use_adapterp: False
train_on_inputs: True
scaling: 1.0
adapter_name: lora
target_modules: None
group_by_length: False
wandb_project: 
wandb_run_name: OpenReasoning-Nemotron-14B-Sparse-0.67-Math14k-part1
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: None
restrict_lora_to_attention: True
speed_preset: fast
log_every: 50
flash_only: False

[peft] Using target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
trainable params: 12,582,912 || all params: 14,782,616,576 || trainable%: 0.0851
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /home/aneek/LLM-Adapters/wandb/offline-run-20251106_135809-479st5wv[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20251106_135809-479st5wv/logs[0m
Finetuning model with params:
base_model: /home/models/nvidia-sparse/OpenReasoning-Nemotron-14B-Sparse-0.67
data_path: /home/aneek/LLM-Adapters/ft-training_set/split_33/math_14k_part2_of_3.json
output_dir: ./trained_models/Nemotron-14B-Sparse/OpenReasoning-Nemotron-14B-Sparse-0.67-Ensemble_split_33/OpenReasoning-Nemotron-14B-Sparse-0.67-Math14k-part2
batch_size: 4
micro_batch_size: 1
num_epochs: 3
learning_rate: 3e-05
cutoff_len: 256
val_set_size: 120
use_gradient_checkpointing: False
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: None
bottleneck_size: 256
non_linearity: tanh
adapter_dropout: 0.0
use_parallel_adapter: False
use_adapterp: False
train_on_inputs: True
scaling: 1.0
adapter_name: lora
target_modules: None
group_by_length: False
wandb_project: 
wandb_run_name: OpenReasoning-Nemotron-14B-Sparse-0.67-Math14k-part2
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: None
restrict_lora_to_attention: True
speed_preset: fast
log_every: 50
flash_only: False

[peft] Using target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
trainable params: 12,582,912 || all params: 14,782,616,576 || trainable%: 0.0851
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /home/aneek/LLM-Adapters/wandb/offline-run-20251106_135837-290bg8km[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20251106_135837-290bg8km/logs[0m
Finetuning model with params:
base_model: /home/models/nvidia-sparse/OpenReasoning-Nemotron-14B-Sparse-0.67
data_path: /home/aneek/LLM-Adapters/ft-training_set/split_33/math_14k_part3_of_3.json
output_dir: ./trained_models/Nemotron-14B-Sparse/OpenReasoning-Nemotron-14B-Sparse-0.67-Ensemble_split_33/OpenReasoning-Nemotron-14B-Sparse-0.67-Math14k-part3
batch_size: 4
micro_batch_size: 1
num_epochs: 3
learning_rate: 3e-05
cutoff_len: 256
val_set_size: 120
use_gradient_checkpointing: False
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: None
bottleneck_size: 256
non_linearity: tanh
adapter_dropout: 0.0
use_parallel_adapter: False
use_adapterp: False
train_on_inputs: True
scaling: 1.0
adapter_name: lora
target_modules: None
group_by_length: False
wandb_project: 
wandb_run_name: OpenReasoning-Nemotron-14B-Sparse-0.67-Math14k-part3
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: None
restrict_lora_to_attention: True
speed_preset: fast
log_every: 50
flash_only: False

[peft] Using target_modules: ['q_proj', 'k_proj', 'v_proj', 'o_proj']
trainable params: 12,582,912 || all params: 14,782,616,576 || trainable%: 0.0851
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /home/aneek/LLM-Adapters/wandb/offline-run-20251106_135906-p3kswdbb[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20251106_135906-p3kswdbb/logs[0m
[DONE] All splits finished for Math14k Ensemble: ./trained_models/Nemotron-14B-Sparse/OpenReasoning-Nemotron-14B-Sparse-0.67-Ensemble_split_33/
